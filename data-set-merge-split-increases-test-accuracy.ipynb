{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Classification with Keras\n",
    "\n",
    "Keras exists to make coding deep neural networks simpler. To demonstrate just how easy it is, you’re going to use Keras to build a convolutional neural network in a few dozen lines of code.\n",
    "\n",
    "You’ll be connecting the concepts from the previous lessons to the methods that Keras provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this run, I will merge the training test set, then split them again using sklearn's train test.  The idea here is that the test set is not representing the training well, that's why our drop in testing.  If we merge, shuffle and split, we should re-distribute the data and get better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement load the data here.\n",
    "with open('train.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "# TODO: Load test data\n",
    "with open('./test.p', mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = data['features']\n",
    "y_train = data['labels']\n",
    "# TODO: Preprocess data & one-hot encode the labels\n",
    "X_test = test['features']\n",
    "y_test = test['labels']\n",
    "\n",
    "X_all = np.concatenate((X_train, X_test), axis = 0)\n",
    "y_all = np.concatenate((y_train, y_test), axis = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.25, stratify = y_all )\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, stratify = y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_val = np_utils.to_categorical(y_val, 43)\n",
    "y_train = np_utils.to_categorical(y_train, 43)\n",
    "y_test = np_utils.to_categorical(y_test, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement data normalization here.\n",
    "def norm(array):\n",
    "    array = array.astype('float32')\n",
    "    array = array / 255 - 0.5\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = norm(X_train)\n",
    "X_val = norm(X_val)\n",
    "X_test = norm(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Build a two-layer feedforward neural network with Keras here.\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop= EarlyStopping(min_delta= 0.008, patience= 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 28800)         0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           3686528     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 43)            5547        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3692971\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, 3, 3, input_shape=(32, 32, 3), activation='relu'))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(43, activation='softmax'))\n",
    "\n",
    "model2.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26048 samples, validate on 12831 samples\n",
      "Epoch 1/20\n",
      "26048/26048 [==============================] - 60s - loss: 1.3258 - acc: 0.6596 - val_loss: 0.5616 - val_acc: 0.8534\n",
      "Epoch 2/20\n",
      "26048/26048 [==============================] - 57s - loss: 0.3587 - acc: 0.9104 - val_loss: 0.2999 - val_acc: 0.9258\n",
      "Epoch 3/20\n",
      "26048/26048 [==============================] - 55s - loss: 0.2102 - acc: 0.9483 - val_loss: 0.2377 - val_acc: 0.9435\n",
      "Epoch 4/20\n",
      "26048/26048 [==============================] - 55s - loss: 0.1332 - acc: 0.9697 - val_loss: 0.1998 - val_acc: 0.9483\n",
      "Epoch 5/20\n",
      "26048/26048 [==============================] - 61s - loss: 0.0964 - acc: 0.9786 - val_loss: 0.1796 - val_acc: 0.9557\n",
      "Epoch 6/20\n",
      "26048/26048 [==============================] - 52s - loss: 0.0744 - acc: 0.9831 - val_loss: 0.1948 - val_acc: 0.9525\n",
      "Epoch 7/20\n",
      "26048/26048 [==============================] - 50s - loss: 0.0680 - acc: 0.9842 - val_loss: 0.1589 - val_acc: 0.9617\n",
      "Epoch 8/20\n",
      "26048/26048 [==============================] - 51s - loss: 0.0513 - acc: 0.9887 - val_loss: 0.1536 - val_acc: 0.9659\n",
      "Epoch 9/20\n",
      "26048/26048 [==============================] - 48s - loss: 0.0397 - acc: 0.9917 - val_loss: 0.1545 - val_acc: 0.9667\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131d15860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val, y_val), callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not a bad test score for a simple model with keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12960/12960 [==============================] - 8s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14014846547219303, 0.96736111111111112]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this section, I will do exactly as above, but I will not merge the train/test from pickle files.  I'll just use them as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Implement load the data here.\n",
    "with open('train.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "# TODO: Load test data\n",
    "with open('./test.p', mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no concat this time\n",
    "\n",
    "X_train = data['features']\n",
    "y_train = data['labels']\n",
    "# TODO: Preprocess data & one-hot encode the labels\n",
    "X_test = test['features']\n",
    "y_test = test['labels']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, stratify = y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val = np_utils.to_categorical(y_val, 43)\n",
    "y_train = np_utils.to_categorical(y_train, 43)\n",
    "y_test = np_utils.to_categorical(y_test, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = norm(X_train)\n",
    "X_val = norm(X_val)\n",
    "X_test = norm(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 59s - loss: 1.3483 - acc: 0.6465 - val_loss: 0.5604 - val_acc: 0.8512\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 59s - loss: 0.3493 - acc: 0.9130 - val_loss: 0.3067 - val_acc: 0.9229\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 53s - loss: 0.1943 - acc: 0.9541 - val_loss: 0.2415 - val_acc: 0.9382\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 50s - loss: 0.1239 - acc: 0.9714 - val_loss: 0.1796 - val_acc: 0.9523\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 55s - loss: 0.0868 - acc: 0.9803 - val_loss: 0.1522 - val_acc: 0.9601\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 58s - loss: 0.0664 - acc: 0.9861 - val_loss: 0.1534 - val_acc: 0.9623\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 58s - loss: 0.0559 - acc: 0.9867 - val_loss: 0.1221 - val_acc: 0.9709\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 59s - loss: 0.0363 - acc: 0.9928 - val_loss: 0.1259 - val_acc: 0.9719\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 54s - loss: 0.0398 - acc: 0.9920 - val_loss: 0.1447 - val_acc: 0.9627\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116bb1e10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(32, 3, 3, input_shape=(32, 32, 3), activation='relu'))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dense(43, activation='softmax'))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val, y_val), callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we see the Test score, it's much lower than my previous test score.  Whats the only difference?  I didn't merge/split the training and testing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630/12630 [==============================] - 7s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.62472940931599574, 0.87727632624519691]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:SDC]",
   "language": "python",
   "name": "conda-env-SDC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
